{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzyPhEnJqs8E"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe==0.10.14 opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uOhJt11qu93"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import base64\n",
        "import json\n",
        "\n",
        "# --- CONFIGURATION: The signs we need ---\n",
        "SIGNS_TO_GENERATE = [\n",
        "    (\"HELLO\", (0, 255, 0)),    # Green Text\n",
        "    (\"THANKS\", (0, 255, 255)), # Yellow Text\n",
        "    (\"YES\", (255, 0, 0)),      # Blue Text\n",
        "    (\"NO\", (0, 0, 255)),       # Red Text\n",
        "    (\"PLEASE\", (255, 0, 255)), # Purple Text\n",
        "    (\"LOVE\", (203, 192, 255)), # Pink Text\n",
        "    (\"PEACE\", (255, 255, 255)),# White Text\n",
        "    (\"YOU\", (100, 255, 100)),\n",
        "    (\"OK\", (100, 100, 255))\n",
        "]\n",
        "\n",
        "def create_flashcard(text, color):\n",
        "    \"\"\"Draws a professional looking flashcard image using code\"\"\"\n",
        "    # 1. Create black background (300x300 pixels)\n",
        "    img = np.zeros((300, 300, 3), dtype=np.uint8)\n",
        "\n",
        "    # 2. Add a colored border\n",
        "    cv2.rectangle(img, (10,10), (290,290), color, 4)\n",
        "\n",
        "    # 3. Add \"SIGN LANGUAGE\" header\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(img, \"SIGN FOR:\", (75, 100), font, 0.8, (150, 150, 150), 2)\n",
        "\n",
        "    # 4. Center the main text\n",
        "    text_size = cv2.getTextSize(text, font, 1.5, 3)[0]\n",
        "    text_x = (300 - text_size[0]) // 2\n",
        "    text_y = 170\n",
        "\n",
        "    # 5. Draw the text\n",
        "    cv2.putText(img, text, (text_x, text_y), font, 1.5, color, 3)\n",
        "\n",
        "    # 6. Convert to Browser Format (Base64)\n",
        "    _, buffer = cv2.imencode('.jpg', img)\n",
        "    img_str = base64.b64encode(buffer).decode('utf-8')\n",
        "    return f\"data:image/jpeg;base64,{img_str}\"\n",
        "\n",
        "print(\"üöÄ Generating Synthetic Database...\")\n",
        "final_db = {}\n",
        "\n",
        "for label, color in SIGNS_TO_GENERATE:\n",
        "    # Key is lowercase because speech recognition returns lowercase\n",
        "    key = label.lower()\n",
        "    final_db[key] = create_flashcard(label, color)\n",
        "    print(f\"   ‚úÖ Created Card: {label}\")\n",
        "\n",
        "# Convert to JSON for the App\n",
        "json_db = json.dumps(final_db)\n",
        "print(\"üéâ Database Ready! (100% Offline & Safe)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3sgsI8D-qE_7"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "import os\n",
        "import base64\n",
        "import time\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "# --- 1. SETUP BRAIN ---\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=False,\n",
        "    max_num_hands=1,\n",
        "    min_detection_confidence=0.7,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "# --- 2. JAVASCRIPT ENGINE (With Sentence Display) ---\n",
        "def video_stream(custom_db):\n",
        "  js = Javascript(f'''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    var sentenceElement;\n",
        "    var speechElement;\n",
        "    var signContainer;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    var signLanguageDB = {custom_db};\n",
        "    var synth = window.speechSynthesis;\n",
        "    var recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\n",
        "    recognition.continuous = false;\n",
        "    recognition.lang = 'en-US';\n",
        "    recognition.interimResults = false;\n",
        "\n",
        "    // --- SPEECH TO SIGN IMAGES (Visual Sentence) ---\n",
        "    recognition.onresult = function(event) {{\n",
        "        var transcript = event.results[0][0].transcript.toLowerCase();\n",
        "        speechElement.innerText = \"Heard: \" + transcript;\n",
        "        speechElement.style.color = \"lime\";\n",
        "\n",
        "        // Clear previous images\n",
        "        signContainer.innerHTML = \"\";\n",
        "\n",
        "        // Split sentence into words and find images\n",
        "        var words = transcript.split(\" \");\n",
        "        var foundAny = false;\n",
        "\n",
        "        words.forEach(word => {{\n",
        "            // Remove punctuation\n",
        "            var cleanWord = word.replace(/[^a-zA-Z]/g, \"\");\n",
        "\n",
        "            // Check if we have an image for this word\n",
        "            // We verify if the key exists in our DB\n",
        "            var match = Object.keys(signLanguageDB).find(key => key === cleanWord);\n",
        "\n",
        "            if (match) {{\n",
        "                var img = document.createElement(\"img\");\n",
        "                img.src = signLanguageDB[match];\n",
        "                img.style.width = \"100px\";\n",
        "                img.style.border = \"2px solid lime\";\n",
        "                img.style.margin = \"5px\";\n",
        "                img.style.borderRadius = \"10px\";\n",
        "                signContainer.appendChild(img);\n",
        "                foundAny = true;\n",
        "            }}\n",
        "        }});\n",
        "\n",
        "        if (!foundAny) {{\n",
        "            signContainer.innerHTML = \"<span style='color:white; background:black; padding:5px;'>No sign images found for this sentence.</span>\";\n",
        "        }}\n",
        "    }};\n",
        "\n",
        "    function speakText(text) {{\n",
        "        if (synth.speaking) return;\n",
        "        var utterThis = new SpeechSynthesisUtterance(text);\n",
        "        utterThis.rate = 0.9;\n",
        "        synth.speak(utterThis);\n",
        "    }}\n",
        "\n",
        "    function removeDom() {{\n",
        "       stream.getVideoTracks().forEach(track => track.stop());\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }}\n",
        "\n",
        "    function onAnimationFrame() {{\n",
        "      if (!shutdown) {{\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }}\n",
        "      if (pendingResolve) {{\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {{\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }}\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }}\n",
        "    }}\n",
        "\n",
        "    async function createDom() {{\n",
        "      if (div !== null) {{\n",
        "        return stream;\n",
        "      }}\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      div.style.position = 'relative';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      // 1. AI STATUS\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>AI Sees:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = '...';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      labelElement.style.color = 'blue';\n",
        "      labelElement.style.fontSize = '20px';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      // 2. SENTENCE BUILDER (New!)\n",
        "      const sentenceOut = document.createElement('div');\n",
        "      sentenceOut.style.backgroundColor = \"#eee\";\n",
        "      sentenceOut.style.padding = \"5px\";\n",
        "      sentenceOut.style.marginTop = \"5px\";\n",
        "      sentenceOut.innerHTML = \"<span>üìù Sentence: </span>\";\n",
        "      sentenceElement = document.createElement('span');\n",
        "      sentenceElement.innerText = '';\n",
        "      sentenceElement.style.fontWeight = 'bold';\n",
        "      sentenceElement.style.color = 'black';\n",
        "      sentenceOut.appendChild(sentenceElement);\n",
        "      div.appendChild(sentenceOut);\n",
        "\n",
        "      // 3. MIC SECTION\n",
        "      const speechOut = document.createElement('div');\n",
        "      speechOut.style.marginTop = '5px';\n",
        "      speechOut.innerHTML = \"<span>Voice:</span>\";\n",
        "      speechElement = document.createElement('span');\n",
        "      speechElement.innerText = '(Click mic to speak)';\n",
        "      speechElement.style.fontWeight = 'bold';\n",
        "      speechElement.style.color = 'gray';\n",
        "      speechOut.appendChild(speechElement);\n",
        "      div.appendChild(speechOut);\n",
        "\n",
        "      const micBtn = document.createElement('button');\n",
        "      micBtn.innerText = \"üé§ Speak Back\";\n",
        "      micBtn.style.margin = \"5px\";\n",
        "      micBtn.onclick = () => {{\n",
        "          speechElement.innerText = \"Listening...\";\n",
        "          speechElement.style.color = \"orange\";\n",
        "          recognition.start();\n",
        "      }};\n",
        "      div.appendChild(micBtn);\n",
        "\n",
        "      // 4. SIGN IMAGE CONTAINER\n",
        "      signContainer = document.createElement('div');\n",
        "      signContainer.style.position = 'absolute';\n",
        "      signContainer.style.bottom = '10px';\n",
        "      signContainer.style.right = '10px';\n",
        "      signContainer.style.zIndex = '100';\n",
        "      signContainer.style.display = 'flex'; // Allow multiple images\n",
        "      div.appendChild(signContainer);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => {{ shutdown = true; }};\n",
        "\n",
        "      stream = await navigator.mediaDevices.getUserMedia({{\n",
        "          video: {{ facingMode: \"environment\"}},\n",
        "          audio: true\n",
        "      }});\n",
        "      video.muted = true;\n",
        "\n",
        "      div.appendChild(video);\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = '<span style=\"color: red;\">CLICK VIDEO TO STOP</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => {{ shutdown = true; }};\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }}\n",
        "\n",
        "    async function stream_frame(label, sentence_text, speak_command) {{\n",
        "      if (shutdown) {{\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }}\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      if (label != \"\") labelElement.innerHTML = label;\n",
        "      if (sentence_text != \"\") sentenceElement.innerHTML = sentence_text;\n",
        "      if (speak_command != \"\") speakText(speak_command);\n",
        "\n",
        "      var result = await new Promise(function(resolve, reject) {{ pendingResolve = resolve; }});\n",
        "      shutdown = false;\n",
        "      return {{'img': result}};\n",
        "    }}\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, sentence_text, speak_command):\n",
        "  js_script = 'stream_frame({}, {}, {})'.format(json.dumps(label), json.dumps(sentence_text), json.dumps(speak_command))\n",
        "  data = eval_js(js_script)\n",
        "  return data\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "def get_dist(p1, p2):\n",
        "    return math.hypot(p1.x - p2.x, p1.y - p2.y)\n",
        "\n",
        "# --- 3. DATABASE (Include your new words here!) ---\n",
        "# Note: You can upload 'you.jpg' or 'ok.jpg' to use them!\n",
        "backup_db = {\n",
        "    \"hello\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Asl_hello.svg/200px-Asl_hello.svg.png\",\n",
        "    \"thanks\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/ASL_sign_Thank_You.jpg/220px-ASL_sign_Thank_You.jpg\",\n",
        "    \"yes\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Asl_y.svg/166px-Asl_y.svg.png\",\n",
        "    \"no\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Asl_n.svg/151px-Asl_n.svg.png\",\n",
        "    \"please\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/ASL_sign_Please.jpg/220px-ASL_sign_Please.jpg\",\n",
        "    \"love\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/I_Love_You_Hand_Symbol.svg/1200px-I_Love_You_Hand_Symbol.svg.png\",\n",
        "    \"peace\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Hand_V_sign.jpg/220px-Hand_V_sign.jpg\",\n",
        "    \"you\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Asl_you.png/220px-Asl_you.png\", # Placeholder\n",
        "    \"ok\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Asl_f9.svg/220px-Asl_f9.svg.png\" # Placeholder\n",
        "}\n",
        "json_db = json.dumps(backup_db)\n",
        "\n",
        "# --- 4. MAIN LOOP ---\n",
        "print(\"Starting Sentence Mode AI...\")\n",
        "video_stream(json_db)\n",
        "\n",
        "label_html = '...'\n",
        "sentence_display = \"\"\n",
        "current_sentence = []\n",
        "last_gesture = \"...\"\n",
        "stable_frames = 0\n",
        "SPEAK_THRESHOLD = 5\n",
        "\n",
        "# Timing for auto-clear\n",
        "last_add_time = time.time()\n",
        "\n",
        "while True:\n",
        "    speak_cmd = \"\"\n",
        "\n",
        "    # Send data to JS\n",
        "    js_reply = video_frame(label_html, sentence_display, speak_cmd)\n",
        "    if not js_reply: break\n",
        "\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    result = hands.process(rgb_img)\n",
        "\n",
        "    current_gesture = \"...\"\n",
        "\n",
        "    if result.multi_hand_landmarks:\n",
        "        for hand_landmarks in result.multi_hand_landmarks:\n",
        "            # GEOMETRY LOGIC\n",
        "            thumb_tip = hand_landmarks.landmark[4]\n",
        "            index_tip = hand_landmarks.landmark[8]\n",
        "            middle_tip = hand_landmarks.landmark[12]\n",
        "            ring_tip = hand_landmarks.landmark[16]\n",
        "            pinky_tip = hand_landmarks.landmark[20]\n",
        "\n",
        "            index_pip = hand_landmarks.landmark[6]\n",
        "            middle_pip = hand_landmarks.landmark[10]\n",
        "            ring_pip = hand_landmarks.landmark[14]\n",
        "            pinky_pip = hand_landmarks.landmark[18]\n",
        "\n",
        "            # Check Folds\n",
        "            index_folded = index_tip.y > index_pip.y\n",
        "            middle_folded = middle_tip.y > middle_pip.y\n",
        "            ring_folded = ring_tip.y > ring_pip.y\n",
        "            pinky_folded = pinky_tip.y > pinky_pip.y\n",
        "\n",
        "            thumb_out = get_dist(thumb_tip, index_pip) > 0.15\n",
        "\n",
        "            # Distance between Thumb and Index (For \"OK\" Sign)\n",
        "            thumb_index_dist = get_dist(thumb_tip, index_tip)\n",
        "\n",
        "            # --- DEFINITIONS ---\n",
        "\n",
        "            # 1. YOU / POINT (Index UP, others DOWN)\n",
        "            if not index_folded and middle_folded and ring_folded and pinky_folded:\n",
        "                current_gesture = \"YOU\"\n",
        "\n",
        "            # 2. HELLO (Palm Open)\n",
        "            elif not index_folded and not middle_folded and not ring_folded and not pinky_folded:\n",
        "                current_gesture = \"HELLO\"\n",
        "\n",
        "            # 3. YES (Fist + Thumb Up) [Wait, 'YOU' and 'YES' conflict? Yes usually has thumb active]\n",
        "            # Let's adjust: YES needs Thumb UP and ABOVE Index Knuckle\n",
        "            elif index_folded and middle_folded and ring_folded and pinky_folded and (thumb_tip.y < index_pip.y):\n",
        "                current_gesture = \"YES\"\n",
        "\n",
        "            # 4. OK (Thumb touches Index, others UP)\n",
        "            elif thumb_index_dist < 0.05 and not middle_folded and not ring_folded and not pinky_folded:\n",
        "                current_gesture = \"OK\"\n",
        "\n",
        "            # 5. I LOVE YOU (Spider-man)\n",
        "            elif not index_folded and middle_folded and ring_folded and not pinky_folded and thumb_out:\n",
        "                current_gesture = \"LOVE\"\n",
        "\n",
        "            # 6. PEACE\n",
        "            elif not index_folded and not middle_folded and ring_folded and pinky_folded:\n",
        "                current_gesture = \"PEACE\"\n",
        "\n",
        "            # 7. CLEAR / STOP (Fist, but thumb tucked in or just plain fist)\n",
        "            elif index_folded and middle_folded and ring_folded and pinky_folded and not (thumb_tip.y < index_pip.y):\n",
        "                current_gesture = \"CLEAR\"\n",
        "\n",
        "    # --- SENTENCE BUILDING LOGIC ---\n",
        "    if current_gesture == last_gesture and current_gesture != \"...\":\n",
        "        stable_frames += 1\n",
        "        if stable_frames == SPEAK_THRESHOLD:\n",
        "\n",
        "            # ACTION: Add word to sentence\n",
        "            if current_gesture == \"CLEAR\":\n",
        "                # If Fist -> Clear sentence\n",
        "                current_sentence = []\n",
        "                speak_cmd = \"Cleared.\"\n",
        "            else:\n",
        "                # Add word only if it's not the same as the very last word (prevent \"Hello Hello\")\n",
        "                if not current_sentence or current_sentence[-1] != current_gesture:\n",
        "                    current_sentence.append(current_gesture)\n",
        "                    speak_cmd = current_gesture # Speak the word as you sign it\n",
        "\n",
        "            # Update Display\n",
        "            sentence_display = \" \".join(current_sentence)\n",
        "            stable_frames = 0 # Reset\n",
        "            last_add_time = time.time()\n",
        "\n",
        "    else:\n",
        "        stable_frames = 0\n",
        "        last_gesture = current_gesture\n",
        "        label_html = f'<span>Seeing: {current_gesture}</span>'\n",
        "\n",
        "    # AUTO-SPEAK SENTENCE: If you stop signing for 4 seconds, say the whole thing\n",
        "    if len(current_sentence) > 0 and (time.time() - last_add_time > 4.0):\n",
        "        full_text = \" \".join(current_sentence)\n",
        "        speak_cmd = f\"Sentence complete. {full_text}\"\n",
        "        current_sentence = [] # Reset after speaking\n",
        "        sentence_display = \"[Sent]\"\n",
        "        last_add_time = time.time()\n",
        "\n",
        "    if speak_cmd != \"\":\n",
        "         video_frame(label_html, sentence_display, speak_cmd)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}